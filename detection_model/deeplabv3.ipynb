{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Import Libraries and Authenticate\n",
        "import os\n",
        "import io\n",
        "import re # For more flexible filename matching\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm # Use notebook version for Colab progress bars\n",
        "from google.colab import auth\n",
        "from google.cloud import storage\n",
        "\n",
        "print(\"Importing libraries...\")\n",
        "\n",
        "print(\"Authenticating to Google Cloud...\")\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    print(\"Authentication successful.\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Authentication failed: {e}\")\n",
        "    # Depending on the error, you might need to restart the Colab runtime\n",
        "    # or check your Google Cloud project permissions.\n",
        "    raise\n",
        "\n",
        "# Initialize GCS client\n",
        "gcs_client = None\n",
        "bucket_name = 'segmentedimages' # <<< YOUR BUCKET NAME HERE\n",
        "gcs_bucket = None\n",
        "try:\n",
        "    gcs_client = storage.Client()\n",
        "    gcs_bucket = gcs_client.bucket(bucket_name)\n",
        "    # Test connection by trying to get bucket metadata\n",
        "    gcs_bucket.reload()\n",
        "    print(f\"GCS Client initialized for bucket: gs://{bucket_name}\")\n",
        "except Exception as e:\n",
        "    print(f\"ERROR: Failed to initialize GCS client or access bucket gs://{bucket_name}. Check bucket name and permissions.\")\n",
        "    print(f\"Error details: {e}\")\n",
        "    gcs_client = None # Ensure client is None if initialization failed"
      ],
      "metadata": {
        "id": "mnapFcOoTvdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2 & 3: Define Data Sources and Component Aggregation (Corrected)\n",
        "\n",
        "# Define the raw source folders for images and masks\n",
        "# Format: (image_gcs_prefix, mask_gcs_prefix)\n",
        "# NOTE: Prefixes should end with '/' to indicate folders\n",
        "data_sources = {\n",
        "    \"labelstudio_busbar_mid\": ('labelstudio/busbar_mid/images/', 'labelstudio/busbar_mid/segmented_images/'),\n",
        "    \"labelstudio_plastic_film_aug\": ('labelstudio/plastic_film/augment/augmented_images/', 'labelstudio/plastic_film/augment/augmented_masks/'),\n",
        "    \"sreeni_bolt1_aug\": ('sreeni/bolt1/augment/augmented_images/', 'sreeni/bolt1/augment/augmented_masks/'),\n",
        "    \"sreeni_bolt2_aug\": ('sreeni/bolt2/augment/augmented_images/', 'sreeni/bolt2/augment/augmented_masks/'),\n",
        "    \"sreeni_bolt3_aug\": ('sreeni/bolt3/augment/augmented_images/', 'sreeni/bolt3/augment/augmented_masks/'),\n",
        "    \"sreeni_cable_aug\": ('sreeni/cable/augment/augmented_images/', 'sreeni/cable/augment/augmented_masks/'),\n",
        "    \"sreeni_connectors_aug\": ('sreeni/connectors/augment/augmented_images/', 'sreeni/connectors/augment/augmented_masks/'),\n",
        "    \"sreeni_nut1_aug\": ('sreeni/nut1/augment/augmented_images/', 'sreeni/nut1/augment/augmented_masks/'),\n",
        "    \"sreeni_nut2_aug\": ('sreeni/nut2/augment/augmented_images/', 'sreeni/nut2/augment/augmented_masks/'),\n",
        "    \"sreeni_plastic_cover_aug\": ('sreeni/plastic_cover/augment/augmented_images/', 'sreeni/plastic_cover/augment/augmented_masks/'),\n",
        "    # --- Corrected sreeni busbar paths ---\n",
        "    \"sreeni_busbar_long\": ('sreeni/busbar_long/images/', 'sreeni/busbar_long/segmented_images/'),\n",
        "    \"sreeni_busbar_mid\": ('sreeni/busbar_mid/images/', 'sreeni/busbar_mid/segmented_images/'),\n",
        "    \"sreeni_busbar_short\": ('sreeni/busbar_short/images/', 'sreeni/busbar_short/segmented_images/'),\n",
        "}\n",
        "\n",
        "# Define how raw sources map to final components\n",
        "component_aggregation = {\n",
        "    \"bolt\": [\"sreeni_bolt1_aug\", \"sreeni_bolt2_aug\", \"sreeni_bolt3_aug\"],\n",
        "    \"busbar\": [\"labelstudio_busbar_mid\", \"sreeni_busbar_long\", \"sreeni_busbar_mid\", \"sreeni_busbar_short\"],\n",
        "    \"cable\": [\"sreeni_cable_aug\"],\n",
        "    \"connector\": [\"sreeni_connectors_aug\"], # Expecting ~900 pairs here now\n",
        "    \"nut\": [\"sreeni_nut1_aug\", \"sreeni_nut2_aug\"],\n",
        "    \"plasticfilm\": [\"labelstudio_plastic_film_aug\"], # Expecting ~700 pairs here\n",
        "    \"plasticcover\": [\"sreeni_plastic_cover_aug\"],\n",
        "}\n",
        "\n",
        "print(\"Data sources (with corrected busbar paths) and component aggregation defined.\")"
      ],
      "metadata": {
        "id": "LCALAAlaT6J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 4: Function to List Files from GCS\n",
        "\n",
        "def list_gcs_files(bucket, prefix):\n",
        "    \"\"\"Lists all files in a GCS bucket with a given prefix.\"\"\"\n",
        "    if not bucket:\n",
        "        print(f\"ERROR: GCS bucket not initialized. Cannot list files for prefix: {prefix}\")\n",
        "        return []\n",
        "    try:\n",
        "        blobs = bucket.list_blobs(prefix=prefix)\n",
        "        # Use full gs:// path for uniqueness, skip folders (blobs ending with '/')\n",
        "        # Convert iterator to list to catch potential errors early and get length\n",
        "        file_list = [f\"gs://{bucket.name}/{blob.name}\" for blob in blobs if not blob.name.endswith('/')]\n",
        "        return file_list\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR listing files in gs://{bucket.name}/{prefix}: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"GCS file listing function defined.\")"
      ],
      "metadata": {
        "id": "Gp2wdTC5UJYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5 & 6: Aggregate File Paths and Match Images/Masks (with Enhanced Debugging)\n",
        "\n",
        "def get_base_filename(gcs_path):\n",
        "    \"\"\"Extracts the base filename for matching purposes, handling variations.\"\"\"\n",
        "    filename = os.path.basename(gcs_path)\n",
        "    # Remove common image extensions first\n",
        "    base = re.sub(r'\\.(jpg|jpeg|png|bmp|tiff)$', '', filename, flags=re.IGNORECASE)\n",
        "    # Then remove common mask suffixes (case-insensitive just in case)\n",
        "    base = re.sub(r'_semantic_mask$', '', base, flags=re.IGNORECASE)\n",
        "    # Add any other specific suffix removals if needed based on examples\n",
        "    # e.g., if some masks end in _mask.png instead of _semantic_mask.png\n",
        "    # base = re.sub(r'_mask$', '', base, flags=re.IGNORECASE)\n",
        "    return base\n",
        "\n",
        "def match_images_masks(image_paths, mask_paths):\n",
        "    \"\"\"Matches image paths to mask paths based on derived base filenames.\"\"\"\n",
        "    # Create map from base mask filename to full mask path\n",
        "    # Use a list to handle potential duplicate base names from masks\n",
        "    mask_map = {}\n",
        "    for p in mask_paths:\n",
        "        base_name = get_base_filename(p)\n",
        "        if base_name not in mask_map:\n",
        "            mask_map[base_name] = []\n",
        "        mask_map[base_name].append(p)\n",
        "\n",
        "    matched_pairs = []\n",
        "    unmatched_images = []\n",
        "    processed_mask_paths = set() # Keep track of masks that HAVE been matched\n",
        "\n",
        "    print(f\"Attempting to match {len(image_paths)} images with {len(mask_paths)} masks...\")\n",
        "\n",
        "    # --- Debugging: Check for duplicate base names derived from images ---\n",
        "    image_base_names = {}\n",
        "    for img_path in image_paths:\n",
        "        base_name = get_base_filename(img_path)\n",
        "        if base_name not in image_base_names:\n",
        "            image_base_names[base_name] = 0\n",
        "        image_base_names[base_name] += 1\n",
        "    duplicate_image_bases = {k: v for k, v in image_base_names.items() if v > 1}\n",
        "    if duplicate_image_bases:\n",
        "        print(f\"\\nWarning: Found {len(duplicate_image_bases)} base filenames derived from multiple images. This could cause issues if masks don't also have duplicates.\")\n",
        "        # print(\"Example duplicate base names (from images):\", list(duplicate_image_bases.keys())[:5])\n",
        "    # --- End Debugging ---\n",
        "\n",
        "    for img_path in tqdm(image_paths, desc=\"Matching images\"):\n",
        "        base_name = get_base_filename(img_path)\n",
        "        if base_name in mask_map:\n",
        "            # Find the best matching mask (simple case: take the first one)\n",
        "            potential_masks = mask_map[base_name]\n",
        "            if potential_masks: # Check if there are still masks available for this base name\n",
        "                mask_path_to_use = potential_masks.pop(0) # Take the first available mask\n",
        "                if not potential_masks: # Remove entry from dict if list is now empty\n",
        "                    del mask_map[base_name]\n",
        "\n",
        "                matched_pairs.append((img_path, mask_path_to_use))\n",
        "                processed_mask_paths.add(mask_path_to_use) # Mark this specific mask path as used\n",
        "            else:\n",
        "                # This case should ideally not happen if initial counts match and base names are unique,\n",
        "                # but could occur if an image base name is duplicated but the mask base name isn't.\n",
        "                print(f\"Warning: Image base name '{base_name}' found in mask_map keys, but the list was empty. Image: {os.path.basename(img_path)}\")\n",
        "                unmatched_images.append(img_path)\n",
        "        else:\n",
        "            unmatched_images.append(img_path)\n",
        "\n",
        "    # Calculate unmatched masks *after* iterating through all images\n",
        "    # These are masks whose full path wasn't added to processed_mask_paths\n",
        "    unmatched_masks = [p for p in mask_paths if p not in processed_mask_paths]\n",
        "\n",
        "    # --- Reporting ---\n",
        "    print(f\"\\nSuccessfully matched {len(matched_pairs)} image/mask pairs.\") # Matches found\n",
        "\n",
        "    if unmatched_images:\n",
        "        print(f\"Warning: Found {len(unmatched_images)} images without corresponding masks.\")\n",
        "        print(\"--> First 5 unmatched image filenames:\")\n",
        "        for i, img_p in enumerate(unmatched_images[:5]):\n",
        "             print(f\"    {i+1}. {os.path.basename(img_p)} (Derived base: '{get_base_filename(img_p)}')\") # Also show derived base\n",
        "\n",
        "    if unmatched_masks:\n",
        "        print(f\"Warning: Found {len(unmatched_masks)} masks without corresponding images.\")\n",
        "        # --- MODIFICATION: Print more details about unmatched masks ---\n",
        "        print(f\"--> First 20 unmatched mask filenames (out of {len(unmatched_masks)}):\")\n",
        "        for i, mask_p in enumerate(unmatched_masks[:20]):\n",
        "             base_name = get_base_filename(mask_p)\n",
        "             print(f\"    {i+1}. {os.path.basename(mask_p)} (Derived base: '{base_name}')\")\n",
        "        if len(unmatched_masks) > 20:\n",
        "             print(\"    ...\")\n",
        "        # --- END MODIFICATION ---\n",
        "\n",
        "    # --- Debugging: Check if any masks had duplicate base names initially ---\n",
        "    duplicate_mask_bases = []\n",
        "    temp_mask_bases = {}\n",
        "    for p in mask_paths:\n",
        "        base_name = get_base_filename(p)\n",
        "        if base_name not in temp_mask_bases:\n",
        "             temp_mask_bases[base_name] = 0\n",
        "        temp_mask_bases[base_name] += 1\n",
        "        if temp_mask_bases[base_name] == 2: # Report only once when it becomes a duplicate\n",
        "             duplicate_mask_bases.append(base_name)\n",
        "    if duplicate_mask_bases:\n",
        "         print(f\"\\nNote: Found {len(duplicate_mask_bases)} base filenames derived from multiple masks initially.\")\n",
        "         # print(\"Example duplicate base names (from masks):\", duplicate_mask_bases[:5])\n",
        "    # --- End Debugging ---\n",
        "\n",
        "    return matched_pairs\n",
        "\n",
        "print(\"File matching function ('match_images_masks') updated with enhanced debugging output.\")"
      ],
      "metadata": {
        "id": "KKQPPfCzUOIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 5b: Run Aggregation and Matching\n",
        "\n",
        "all_component_data = {}\n",
        "total_matched_pairs = 0\n",
        "\n",
        "if gcs_client and gcs_bucket: # Proceed only if GCS connection is okay\n",
        "    print(\"\\nStarting file aggregation and matching process...\")\n",
        "\n",
        "    for component, source_keys in component_aggregation.items():\n",
        "        print(f\"\\nProcessing component: {component.upper()}\")\n",
        "        component_image_paths = []\n",
        "        component_mask_paths = []\n",
        "        valid_component = True\n",
        "\n",
        "        for key in source_keys:\n",
        "            if key not in data_sources:\n",
        "                print(f\"  Warning: Source key '{key}' not found in data_sources definition. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            img_prefix, mask_prefix = data_sources[key]\n",
        "            print(f\"  Listing files from gs://{bucket_name}/{img_prefix} and gs://{bucket_name}/{mask_prefix}\")\n",
        "\n",
        "            # List images\n",
        "            current_image_paths = list_gcs_files(gcs_bucket, img_prefix)\n",
        "            if not current_image_paths and img_prefix: # Avoid warning if prefix was intentionally empty\n",
        "                 print(f\"    Warning: No image files found in gs://{bucket_name}/{img_prefix}\")\n",
        "                 # Decide if this is critical - maybe set valid_component = False?\n",
        "            component_image_paths.extend(current_image_paths)\n",
        "            print(f\"    Found {len(current_image_paths)} images.\")\n",
        "\n",
        "            # List masks\n",
        "            current_mask_paths = list_gcs_files(gcs_bucket, mask_prefix)\n",
        "            if not current_mask_paths and mask_prefix:\n",
        "                 print(f\"    Warning: No mask files found in gs://{bucket_name}/{mask_prefix}\")\n",
        "                 # Decide if this is critical\n",
        "            component_mask_paths.extend(current_mask_paths)\n",
        "            print(f\"    Found {len(current_mask_paths)} masks.\")\n",
        "\n",
        "            # Basic check for equal numbers in this sub-source (optional but helpful)\n",
        "            # if len(current_image_paths) != len(current_mask_paths):\n",
        "            #     print(f\"    Warning: Mismatch in count for source '{key}'. Images: {len(current_image_paths)}, Masks: {len(current_mask_paths)}\")\n",
        "\n",
        "\n",
        "        if not component_image_paths or not component_mask_paths:\n",
        "             print(f\"  Skipping matching for {component} due to missing images or masks in its sources.\")\n",
        "             all_component_data[component] = [] # Store empty list for this component\n",
        "             continue # Skip to next component\n",
        "\n",
        "        print(f\"  Total raw images listed for {component}: {len(component_image_paths)}\")\n",
        "        print(f\"  Total raw masks listed for {component}: {len(component_mask_paths)}\")\n",
        "\n",
        "        # Match images and masks for the current aggregated component\n",
        "        matched_pairs = match_images_masks(component_image_paths, component_mask_paths)\n",
        "        all_component_data[component] = matched_pairs\n",
        "        total_matched_pairs += len(matched_pairs)\n",
        "        print(f\"  Finished processing {component}. Matched pairs stored: {len(matched_pairs)}\")\n",
        "\n",
        "    print(f\"\\n--- Data Aggregation and Matching Complete ---\")\n",
        "    print(f\"Total matched image/mask pairs across all components: {total_matched_pairs}\")\n",
        "\n",
        "    # Print summary per component\n",
        "    print(\"\\nMatched pairs per component:\")\n",
        "    for component, pairs in all_component_data.items():\n",
        "        print(f\"- {component}: {len(pairs)}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nERROR: GCS client/bucket not initialized. Cannot proceed with file aggregation.\")\n",
        "    print(\"Please check authentication and bucket name in Step 1.\")"
      ],
      "metadata": {
        "id": "SSAsB76eUtZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 7: Define Label Remapping\n",
        "\n",
        "# Original labels: background-0, bolt-1, busbar-3, cable-4, connector-5, nut-6, plasticfilm-7, plasticcover-8\n",
        "# Remapped labels: background-0, bolt-1, busbar-2, cable-3, connector-4, nut-5, plasticfilm-6, plasticcover-7\n",
        "# Class ID 2 is assumed to be the unused/removed component.\n",
        "\n",
        "original_to_remapped_dict = {\n",
        "    0: 0,  # background -> background\n",
        "    1: 1,  # bolt -> bolt\n",
        "    # Original class 2 is missing/removed\n",
        "    3: 2,  # busbar -> busbar\n",
        "    4: 3,  # cable -> cable\n",
        "    5: 4,  # connector -> connector\n",
        "    6: 5,  # nut -> nut\n",
        "    7: 6,  # plasticfilm -> plasticfilm\n",
        "    8: 7   # plasticcover -> plasticcover\n",
        "}\n",
        "\n",
        "# Create a lookup array for efficiency (vectorized remapping)\n",
        "# Find the maximum original label ID that *needs* mapping\n",
        "max_original_label = 0\n",
        "if original_to_remapped_dict: # Check if dictionary is not empty\n",
        "    max_original_label = max(original_to_remapped_dict.keys())\n",
        "\n",
        "# Initialize lookup table with a default value (e.g., 0 for background or an unused value)\n",
        "# Size needs to be max_original_label + 1 to include index max_original_label\n",
        "# Using 0 as default: if an unexpected label appears, map it to background.\n",
        "lookup_table = np.zeros(max_original_label + 1, dtype=np.uint8)\n",
        "for original_id, remapped_id in original_to_remapped_dict.items():\n",
        "    lookup_table[original_id] = remapped_id\n",
        "\n",
        "print(\"\\nLabel remapping dictionary and lookup table created:\")\n",
        "print(\"Original -> Remapped Dictionary:\", original_to_remapped_dict)\n",
        "# print(\"Lookup Table (Index is Original ID, Value is Remapped ID):\") # Can be long\n",
        "# print(lookup_table)"
      ],
      "metadata": {
        "id": "MGCThu0zY1su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 8: Create Mask Remapping Function\n",
        "\n",
        "def remap_mask_labels(mask_array, lookup_table):\n",
        "    \"\"\"Remaps labels in a mask array using a lookup table.\"\"\"\n",
        "    # Ensure mask_array is numpy array\n",
        "    if not isinstance(mask_array, np.ndarray):\n",
        "        raise TypeError(\"Input mask must be a NumPy array.\")\n",
        "    if mask_array.size == 0:\n",
        "         print(\"Warning: Received an empty mask array. Returning empty array.\")\n",
        "         return mask_array # Return empty array if input is empty\n",
        "\n",
        "    # Check if any values in the mask are outside the bounds of the lookup table\n",
        "    max_val_in_mask = np.max(mask_array)\n",
        "    max_supported_label = len(lookup_table) - 1\n",
        "\n",
        "    if max_val_in_mask > max_supported_label:\n",
        "         print(f\"Warning: Mask contains value ({max_val_in_mask}) larger than max original label supported by lookup table ({max_supported_label}).\")\n",
        "         # Identify problematic pixels\n",
        "         problematic_pixels = mask_array > max_supported_label\n",
        "         unique_problematic_values = np.unique(mask_array[problematic_pixels])\n",
        "         print(f\"  Unique problematic values found: {unique_problematic_values}\")\n",
        "         print(f\"  These pixels will be mapped to lookup_table[0] ({lookup_table[0]}) due to current implementation.\") # Or error if numpy raises index error\n",
        "         # Option: Clip values before lookup (maps high values to max supported remapped value)\n",
        "         # mask_array_clipped = np.clip(mask_array, 0, max_supported_label)\n",
        "         # remapped_mask = lookup_table[mask_array_clipped]\n",
        "         # Option: Map problematic pixels to a specific value (e.g., 0 for background) after lookup\n",
        "         remapped_mask = lookup_table[np.clip(mask_array, 0, max_supported_label)] # Apply lookup on clipped values\n",
        "         remapped_mask[problematic_pixels] = 0 # Force problematic pixels to background\n",
        "         print(f\"  Forcing these pixel values to 0 (background).\")\n",
        "         return remapped_mask\n",
        "         # Option: Raise error\n",
        "         # raise ValueError(f\"Mask contains value ({max_val_in_mask}) outside lookup table range [0, {max_supported_label}]. Problematic values: {unique_problematic_values}\")\n",
        "\n",
        "    # Apply the lookup table - this is efficient vectorized remapping\n",
        "    remapped_mask = lookup_table[mask_array]\n",
        "    return remapped_mask\n",
        "\n",
        "print(\"Mask remapping function defined.\")"
      ],
      "metadata": {
        "id": "s81qqZ09Y-AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 9: Define Loading Functions\n",
        "\n",
        "def load_image_from_gcs(gcs_path, bucket):\n",
        "    \"\"\"Loads an image from GCS path into a PIL Image object.\"\"\"\n",
        "    if not bucket:\n",
        "        print(f\"ERROR: GCS bucket not initialized. Cannot load image {gcs_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        blob_name = gcs_path.replace(f\"gs://{bucket.name}/\", \"\")\n",
        "        blob = bucket.blob(blob_name)\n",
        "        content = blob.download_as_bytes()\n",
        "        img = Image.open(io.BytesIO(content))\n",
        "        # Ensure image is in RGB format (common requirement for models)\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {gcs_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_mask_from_gcs(gcs_path, bucket):\n",
        "    \"\"\"Loads a mask from GCS path into a NumPy array.\"\"\"\n",
        "    if not bucket:\n",
        "        print(f\"ERROR: GCS bucket not initialized. Cannot load mask {gcs_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        blob_name = gcs_path.replace(f\"gs://{bucket.name}/\", \"\")\n",
        "        blob = bucket.blob(blob_name)\n",
        "        content = blob.download_as_bytes()\n",
        "        # Assume masks are grayscale (single channel).\n",
        "        # 'L' mode is 8-bit pixels, black and white.\n",
        "        mask_pil = Image.open(io.BytesIO(content)).convert('L')\n",
        "        return np.array(mask_pil)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading mask {gcs_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"Image and mask loading functions defined.\")"
      ],
      "metadata": {
        "id": "qQLVJr4AZBho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 10: Example - Loading and Remapping a Mask (Optional)\n",
        "\n",
        "# --- Select a component to test (if data exists) ---\n",
        "test_component = 'cable' # Change to 'busbar', 'cable', etc. if needed\n",
        "\n",
        "if gcs_client and gcs_bucket and test_component in all_component_data and all_component_data[test_component]:\n",
        "    print(f\"\\n--- Example: Loading and Remapping first '{test_component}' mask ---\")\n",
        "    # Get the first matched pair for the test component\n",
        "    example_img_path, example_mask_path = all_component_data[test_component][0]\n",
        "\n",
        "    print(f\"Loading image: {example_img_path}\")\n",
        "    example_image = load_image_from_gcs(example_img_path, gcs_bucket)\n",
        "\n",
        "    print(f\"Loading mask: {example_mask_path}\")\n",
        "    original_mask_array = load_mask_from_gcs(example_mask_path, gcs_bucket)\n",
        "\n",
        "    if original_mask_array is not None:\n",
        "        print(\"\\nOriginal mask properties:\")\n",
        "        print(\"  - Shape:\", original_mask_array.shape)\n",
        "        print(\"  - Data type:\", original_mask_array.dtype)\n",
        "        original_unique_labels = np.unique(original_mask_array)\n",
        "        print(\"  - Unique labels found:\", original_unique_labels)\n",
        "\n",
        "        # Remap the labels\n",
        "        print(\"\\nRemapping mask labels...\")\n",
        "        remapped_mask_array = remap_mask_labels(original_mask_array, lookup_table)\n",
        "\n",
        "        print(\"\\nRemapped mask properties:\")\n",
        "        print(\"  - Shape:\", remapped_mask_array.shape)\n",
        "        print(\"  - Data type:\", remapped_mask_array.dtype)\n",
        "        remapped_unique_labels = np.unique(remapped_mask_array)\n",
        "        print(\"  - Unique labels found:\", remapped_unique_labels)\n",
        "\n",
        "        # Verification printout\n",
        "        print(\"\\nVerification (Original Label -> Remapped Label):\")\n",
        "        max_supported_label = len(lookup_table) - 1\n",
        "        labels_to_verify = sorted(list(original_unique_labels))\n",
        "        for label in labels_to_verify:\n",
        "             if label <= max_supported_label:\n",
        "                 print(f\"  {label} -> {lookup_table[label]}\")\n",
        "             else:\n",
        "                 # How it was handled depends on the logic in remap_mask_labels\n",
        "                 print(f\"  {label} -> (Original value > {max_supported_label}, mapped according to warning logic, likely 0)\")\n",
        "\n",
        "        # --- Optional: Display using Matplotlib ---\n",
        "        try:\n",
        "            import matplotlib.pyplot as plt\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "            # Image\n",
        "            if example_image:\n",
        "                axes[0].imshow(example_image)\n",
        "                axes[0].set_title(f\"Original Image\\n{os.path.basename(example_img_path)}\")\n",
        "            else:\n",
        "                 axes[0].set_title(\"Image Load Failed\")\n",
        "            axes[0].axis('off')\n",
        "\n",
        "            # Original Mask\n",
        "            im1 = axes[1].imshow(original_mask_array, cmap='tab20', vmin=0, vmax=max(original_unique_labels.max(), max_original_label, 1)) # Use a colormap, adjust max value\n",
        "            axes[1].set_title(f\"Original Mask\\nLabels: {original_unique_labels}\")\n",
        "            axes[1].axis('off')\n",
        "            # fig.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04) # Optional colorbar\n",
        "\n",
        "            # Remapped Mask\n",
        "            im2 = axes[2].imshow(remapped_mask_array, cmap='tab20', vmin=0, vmax=max(remapped_unique_labels.max(), np.max(lookup_table), 1)) # Adjust max value for remapped labels\n",
        "            axes[2].set_title(f\"Remapped Mask\\nLabels: {remapped_unique_labels}\")\n",
        "            axes[2].axis('off')\n",
        "            # fig.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04) # Optional colorbar\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        except ImportError:\n",
        "            print(\"\\nMatplotlib not found. Cannot display images/masks.\")\n",
        "        except Exception as display_e:\n",
        "             print(f\"\\nError during display: {display_e}\")\n",
        "        # --- End Optional Display ---\n",
        "\n",
        "    else:\n",
        "        print(f\"Failed to load the example mask: {example_mask_path}\")\n",
        "elif not (gcs_client and gcs_bucket):\n",
        "     print(\"\\nSkipping example: GCS client/bucket not initialized.\")\n",
        "elif test_component not in all_component_data:\n",
        "     print(f\"\\nSkipping example: Component '{test_component}' not found in aggregation results.\")\n",
        "elif not all_component_data[test_component]:\n",
        "      print(f\"\\nSkipping example: No matched pairs found for component '{test_component}'.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Full Data Preparation Script Finished ---\")\n",
        "print(\"Variable 'all_component_data' holds the matched (image_path, mask_path) pairs for each component.\")\n",
        "print(\"Use 'load_image_from_gcs' and 'load_mask_from_gcs' to load data.\")\n",
        "print(\"Use 'remap_mask_labels(mask_array, lookup_table)' to remap loaded masks.\")\n",
        "print(\"Review any WARNING or ERROR messages above.\")"
      ],
      "metadata": {
        "id": "4VaeAIUJZIHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "RlQ1PTK3eZ1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 11: Install Necessary Libraries (Pin Versions, Uninstall Conflicts - FOR FRESH RUNTIME)\n",
        "\n",
        "# --- Uninstall potentially conflicting packages FIRST (less critical on fresh runtime, but good practice) ---\n",
        "print(\"Uninstalling potentially conflicting packages (tensorflow, numba, pytensor)...\")\n",
        "# Use -y to automatically confirm uninstallation\n",
        "!pip uninstall tensorflow numba pytensor -y -q\n",
        "\n",
        "# --- Install desired PyTorch versions (Pinned) and dependencies ---\n",
        "# Checked nvcc --version output, which reported CUDA 12.5.\n",
        "# Using the cu121 index URL for PyTorch builds compatible with CUDA 12.x.\n",
        "CUDA_VERSION_SUFFIX = \"cu121\"\n",
        "TORCH_VERSION = \"2.3.1\"       # Known stable version\n",
        "TORCHVISION_VERSION = \"0.18.1\" # Compatible with torch 2.3.1\n",
        "TORCHAUDIO_VERSION = \"2.3.1\"    # Compatible with torch 2.3.1\n",
        "\n",
        "print(f\"\\nAttempting PyTorch install for index: {CUDA_VERSION_SUFFIX} (Pinned Versions)\")\n",
        "print(f\"Torch: {TORCH_VERSION}, Torchvision: {TORCHVISION_VERSION}, Torchaudio: {TORCHAUDIO_VERSION}\")\n",
        "\n",
        "# Use regular install on fresh runtime, no-cache is still good\n",
        "!pip install --no-cache-dir torch=={TORCH_VERSION} torchvision=={TORCHVISION_VERSION} torchaudio=={TORCHAUDIO_VERSION} --index-url https://download.pytorch.org/whl/{CUDA_VERSION_SUFFIX} -q\n",
        "# Install the other dependencies\n",
        "!pip install --no-cache-dir albumentations -q\n",
        "!pip install --no-cache-dir torchmetrics -q\n",
        "!pip install --no-cache-dir tqdm -q\n",
        "\n",
        "print(f\"\\nLibraries installed/checked for target CUDA suffix: {CUDA_VERSION_SUFFIX} with pinned versions.\")\n",
        "\n",
        "# --- Check if core libraries load ---\n",
        "print(\"\\nPerforming post-install check...\")\n",
        "INSTALL_SUCCESS = False # Default to False\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    import torchaudio # Check torchaudio too\n",
        "    import albumentations\n",
        "    import torchmetrics\n",
        "    print(f\"Torch version: {torch.__version__} (Target: {TORCH_VERSION})\")\n",
        "    print(f\"Torchvision version: {torchvision.__version__} (Target: {TORCHVISION_VERSION})\")\n",
        "    print(f\"Torchaudio version: {torchaudio.__version__} (Target: {TORCHAUDIO_VERSION})\")\n",
        "    # Check the CUDA version torch *thinks* it was built with\n",
        "    print(f\"Torch CUDA version detected by torch: {torch.version.cuda}\")\n",
        "\n",
        "    # Check compatibility\n",
        "    print(\"Running internal compatibility check...\")\n",
        "    device_check = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    if torch.cuda.is_available():\n",
        "        # *** CORRECTED roi_align CALL with dtype=torch.float32 ***\n",
        "        torchvision.ops.roi_align(torch.rand(1, 1, 10, 10, device=device_check), torch.tensor([[0, 0, 0, 5, 5]], dtype=torch.float32, device=device_check), output_size=1)\n",
        "        print(\"Internal CUDA compatibility check passed.\")\n",
        "    else:\n",
        "         print(\"Skipping CUDA check as no GPU is available.\")\n",
        "\n",
        "\n",
        "    # Explicit version check\n",
        "    if torch.__version__.startswith(TORCH_VERSION) and \\\n",
        "       torchvision.__version__.startswith(TORCHVISION_VERSION) and \\\n",
        "       torchaudio.__version__.startswith(TORCHAUDIO_VERSION):\n",
        "        print(\">>> Basic Torch/Torchvision/Torchaudio version check PASSED. <<<\")\n",
        "        # Check if torch CUDA version matches target (12.1 for cu121)\n",
        "        if torch.version.cuda is not None and torch.version.cuda.startswith(\"12.1\"):\n",
        "             print(f\">>> Torch CUDA version ({torch.version.cuda}) matches target index ({CUDA_VERSION_SUFFIX}). <<<\")\n",
        "             INSTALL_SUCCESS = True\n",
        "        elif torch.version.cuda is None and not torch.cuda.is_available():\n",
        "             print(\">>> Torch CPU version installed, no CUDA check needed. <<<\")\n",
        "             INSTALL_SUCCESS = True # Allow CPU install to succeed\n",
        "        else:\n",
        "             print(f\"!!! Version Check FAILED: Torch CUDA version ({torch.version.cuda}) does NOT match target index ({CUDA_VERSION_SUFFIX}). !!!\")\n",
        "             INSTALL_SUCCESS = False\n",
        "    else:\n",
        "        print(\"!!! Version Check FAILED: Installed versions do not match pinned targets. !!!\")\n",
        "        INSTALL_SUCCESS = False\n",
        "\n",
        "except RuntimeError as e:\n",
        "    # Catch the specific error if it still happens, or other runtime errors\n",
        "    print(f\"!!! Compatibility Check FAILED (RuntimeError): {e} !!!\")\n",
        "    INSTALL_SUCCESS = False\n",
        "except ImportError as e:\n",
        "    print(f\"!!! Failed to import one of the core libraries: {e} !!!\")\n",
        "    INSTALL_SUCCESS = False\n",
        "except Exception as e:\n",
        "     print(f\"!!! An unexpected error occurred during the check: {e} !!!\")\n",
        "     INSTALL_SUCCESS = False\n",
        "\n",
        "\n",
        "if INSTALL_SUCCESS:\n",
        "    print(\"\\n**************************************************************************************\")\n",
        "    print(\">>> ACTION REQUIRED: Installation and checks successful. Please go to 'Runtime' -> 'Restart runtime' NOW. <<<\")\n",
        "    print(\">>> After restarting, re-run all cells sequentially from Step 1 onwards. <<<\")\n",
        "    print(\"**************************************************************************************\")\n",
        "else:\n",
        "    print(\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "    print(\">>> WARNING: Installation failed or basic checks did not pass. <<<\")\n",
        "    print(\">>> DO NOT RESTART RUNTIME YET. Review the errors above. <<<\")\n",
        "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
      ],
      "metadata": {
        "id": "x7ZTpZa1Zb2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 12: Import Training Libraries and Setup Device\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
        "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights # Alternative model\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2 # OpenCV is needed by albumentations for some transforms\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "# Import metrics - should be installed from Step 11\n",
        "import torchmetrics\n",
        "\n",
        "print(\"Importing core libraries...\")\n",
        "\n",
        "# Setup device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    print(f\"Using GPU: {gpu_name}\")\n",
        "    # Verify CUDA is working with torch\n",
        "    try:\n",
        "        _ = torch.tensor([1.0, 2.0]).cuda()\n",
        "        print(\"CUDA device test successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"!!! CUDA device test FAILED: {e} !!!\")\n",
        "        print(\"!!! Training will likely fail. Check installation and runtime type. !!!\")\n",
        "        # Optionally raise an error: raise RuntimeError(\"CUDA not working\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Warning: Using CPU. Training will be very slow.\")\n",
        "\n",
        "# Ensure previous helper functions and data structures are available\n",
        "# (These should be loaded by re-running cells 1-10 after restart)\n",
        "required_vars = ['all_component_data', 'gcs_bucket', 'lookup_table',\n",
        "                 'load_image_from_gcs', 'load_mask_from_gcs', 'remap_mask_labels']\n",
        "missing_vars = [var for var in required_vars if var not in globals()]\n",
        "if missing_vars:\n",
        "     raise NameError(f\"Prerequisite variables/functions not defined: {', '.join(missing_vars)}. \"\n",
        "                     \"Please run cells 1-10 sequentially after restarting the runtime.\")\n",
        "else:\n",
        "    print(\"Prerequisite variables and functions seem available.\")\n",
        "\n",
        "print(\"Imports complete, device configured.\")"
      ],
      "metadata": {
        "id": "buE1GvgxdK5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 13: Training Configuration / Hyperparameters (with Test Split & Early Stopping)\n",
        "\n",
        "# --- Basic Configuration ---\n",
        "NUM_CLASSES = 8\n",
        "IMG_HEIGHT = 480\n",
        "IMG_WIDTH = 640\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 3 # Maximum number of epochs\n",
        "RANDOM_SEED = 42\n",
        "CHECKPOINT_PATH = \"/content/best_segmentation_model.pth\"\n",
        "\n",
        "# --- Split Proportions ---\n",
        "VAL_SPLIT_FROM_ALL = 0.15\n",
        "TEST_SPLIT_FROM_ALL = 0.15\n",
        "print(f\"Data Split: Train=~{(1.0 - VAL_SPLIT_FROM_ALL - TEST_SPLIT_FROM_ALL)*100:.0f}%, Validation={VAL_SPLIT_FROM_ALL*100:.0f}%, Test={TEST_SPLIT_FROM_ALL*100:.0f}%\")\n",
        "\n",
        "# --- Learning Parameters ---\n",
        "LEARNING_RATE = 1e-4\n",
        "OPTIMIZER_WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# --- Model Selection ---\n",
        "MODEL_ARCH = 'deeplabv3_resnet50'\n",
        "\n",
        "# --- Optional: Learning Rate Scheduler ---\n",
        "USE_LR_SCHEDULER = True\n",
        "SCHEDULER_PATIENCE = 2\n",
        "SCHEDULER_FACTOR = 0.1\n",
        "\n",
        "# --- Optional: Early Stopping ---\n",
        "USE_EARLY_STOPPING = True # Enable/disable early stopping\n",
        "EARLY_STOPPING_PATIENCE = 2 # Number of epochs to wait for val_loss improvement before stopping\n",
        "EARLY_STOPPING_MIN_DELTA = 0.0001 # Minimum change in val_loss to be considered an improvement\n",
        "\n",
        "# --- Optional: Class Weights for Imbalanced Data ---\n",
        "USE_CLASS_WEIGHTING = False\n",
        "\n",
        "print(\"\\nConfiguration set:\")\n",
        "print(f\"  Model: {MODEL_ARCH}\")\n",
        "print(f\"  Num Classes: {NUM_CLASSES}\")\n",
        "print(f\"  Image Size: {IMG_HEIGHT}x{IMG_WIDTH}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Max Epochs: {NUM_EPOCHS}\") # Changed label slightly\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Use LR Scheduler: {USE_LR_SCHEDULER}\")\n",
        "print(f\"  Use Early Stopping: {USE_EARLY_STOPPING} (Patience={EARLY_STOPPING_PATIENCE}, Min Delta={EARLY_STOPPING_MIN_DELTA})\") # Added info\n",
        "print(f\"  Use Class Weights: {USE_CLASS_WEIGHTING}\")\n",
        "\n",
        "# --- Prepare list of all image/mask pairs ---\n",
        "# (Splitting logic remains the same as the version with the test set)\n",
        "# ... (rest of the splitting code from the previous version) ...\n",
        "# --- Prepare list of all image/mask pairs ---\n",
        "# Assumes all_component_data is loaded from previous steps\n",
        "all_pairs = []\n",
        "for component, pairs in all_component_data.items():\n",
        "    all_pairs.extend(pairs)\n",
        "\n",
        "if not all_pairs:\n",
        "    raise ValueError(\"No image/mask pairs found in 'all_component_data'. Check data loading steps (Cells 1-10).\")\n",
        "\n",
        "print(f\"\\nTotal image/mask pairs found: {len(all_pairs)}\")\n",
        "\n",
        "# --- Split data into Training, Validation, and Test sets ---\n",
        "# Attempt stratification by component name extracted from path (heuristic)\n",
        "try:\n",
        "    # Extract component name assuming path like gs://.../component_name/...\n",
        "    stratify_labels = [pair[0].split('/')[3] for pair in all_pairs] # Adjust index if path structure differs\n",
        "    print(\"Using component names for stratification.\")\n",
        "except IndexError:\n",
        "    print(\"Could not extract component names reliably for stratification, using None.\")\n",
        "    stratify_labels = None\n",
        "\n",
        "# 1. First split into Training+Validation and Test\n",
        "try:\n",
        "    train_val_pairs, test_pairs = train_test_split(\n",
        "        all_pairs,\n",
        "        test_size=TEST_SPLIT_FROM_ALL, # Reserve test set first\n",
        "        random_state=RANDOM_SEED,\n",
        "        stratify=stratify_labels if len(all_pairs) > 1 else None # Attempt stratification\n",
        "    )\n",
        "    if stratify_labels: print(\"Attempted stratification by component for train/test split.\")\n",
        "except ValueError as e:\n",
        "     print(f\"Stratification failed for train/test ({e}), using non-stratified split.\")\n",
        "     train_val_pairs, test_pairs = train_test_split(\n",
        "        all_pairs,\n",
        "        test_size=TEST_SPLIT_FROM_ALL,\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "\n",
        "\n",
        "# 2. Calculate split ratio needed for validation from the remaining data\n",
        "remaining_data_prop = 1.0 - TEST_SPLIT_FROM_ALL\n",
        "if remaining_data_prop <= 0: # Avoid division by zero if test split is 100%\n",
        "     val_split_from_remaining = 0\n",
        "     print(\"Warning: Test split is 100% or more, validation set will be empty.\")\n",
        "elif VAL_SPLIT_FROM_ALL >= remaining_data_prop: # Avoid split > 100%\n",
        "     val_split_from_remaining = 1.0\n",
        "     print(\"Warning: Test and Validation splits meet or exceed total data, validation set will use all remaining data.\")\n",
        "else:\n",
        "    val_split_from_remaining = VAL_SPLIT_FROM_ALL / remaining_data_prop\n",
        "\n",
        "# 3. Split Training+Validation into actual Training and Validation\n",
        "if len(train_val_pairs) > 0 and val_split_from_remaining > 0:\n",
        "    try:\n",
        "         # Attempt stratification on the remaining data\n",
        "         stratify_labels_train_val = [pair[0].split('/')[3] for pair in train_val_pairs] if stratify_labels else None\n",
        "         train_pairs, val_pairs = train_test_split(\n",
        "            train_val_pairs,\n",
        "            test_size=val_split_from_remaining, # Split the remainder\n",
        "            random_state=RANDOM_SEED, # Use same random state for consistency\n",
        "            stratify=stratify_labels_train_val if len(train_val_pairs) > 1 else None # Attempt stratification\n",
        "         )\n",
        "         if stratify_labels_train_val: print(\"Attempted stratification by component for train/validation split.\")\n",
        "    except ValueError as e:\n",
        "         print(f\"Stratification failed for train/validation ({e}), using non-stratified split.\")\n",
        "         train_pairs, val_pairs = train_test_split(\n",
        "            train_val_pairs,\n",
        "            test_size=val_split_from_remaining,\n",
        "            random_state=RANDOM_SEED\n",
        "         )\n",
        "    except IndexError: # Handle potential index error during stratification label extraction\n",
        "         print(\"Stratification failed for train/validation due to path format, using non-stratified split.\")\n",
        "         train_pairs, val_pairs = train_test_split(\n",
        "            train_val_pairs,\n",
        "            test_size=val_split_from_remaining,\n",
        "            random_state=RANDOM_SEED\n",
        "         )\n",
        "\n",
        "elif len(train_val_pairs) > 0: # No validation split needed/possible\n",
        "     train_pairs = train_val_pairs\n",
        "     val_pairs = []\n",
        "     print(\"Validation split resulted in zero samples.\")\n",
        "else: # No training/validation data left\n",
        "     train_pairs = []\n",
        "     val_pairs = []\n",
        "     print(\"Train/Validation split resulted in zero samples.\")\n",
        "\n",
        "\n",
        "print(f\"\\nFinal Split Sizes:\")\n",
        "print(f\"  Training set size: {len(train_pairs)}\")\n",
        "print(f\"  Validation set size: {len(val_pairs)}\")\n",
        "print(f\"  Test set size: {len(test_pairs)}\")\n",
        "\n",
        "# Basic check\n",
        "total_split = len(train_pairs) + len(val_pairs) + len(test_pairs)\n",
        "if total_split != len(all_pairs):\n",
        "     print(f\"\\nWarning: Split counts ({total_split}) do not sum to total pairs ({len(all_pairs)}). Check split logic.\")"
      ],
      "metadata": {
        "id": "Fqe7NBPEoiMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 14: Define Data Augmentations (Albumentations)\n",
        "\n",
        "# Normalization stats for models pre-trained on ImageNet\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Define transformations for training (with augmentation)\n",
        "train_transform = A.Compose([\n",
        "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, interpolation=cv2.INTER_NEAREST), # Use INTER_NEAREST for masks\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    # Add more augmentations if needed:\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0), # mask_value=0 for background\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    # A.GaussNoise(p=0.2), # Optional\n",
        "    # A.CoarseDropout(max_holes=8, max_height=IMG_HEIGHT//10, max_width=IMG_WIDTH//10, min_holes=1, fill_value=0, mask_fill_value=0, p=0.3), # Optional\n",
        "    A.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "    ToTensorV2(), # Converts image (HWC -> CHW) and mask (HW -> HW) and scales image to [0,1]\n",
        "])\n",
        "\n",
        "# Define transformations for validation (only resize, normalize, and convert to tensor)\n",
        "val_transform = A.Compose([\n",
        "    A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, interpolation=cv2.INTER_NEAREST),\n",
        "    A.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "    ToTensorV2(),\n",
        "])\n",
        "\n",
        "print(\"Data transformations defined.\")"
      ],
      "metadata": {
        "id": "6EpHlw6krd6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 15: Define Custom Dataset Class (Corrected Index Check)\n",
        "\n",
        "class BatteryComponentDataset(Dataset):\n",
        "    # (Dataset class definition includes robust error handling)\n",
        "    def __init__(self, image_mask_pairs, gcs_bucket_obj, lookup_table, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_mask_pairs (list): List of tuples (image_gcs_path, mask_gcs_path).\n",
        "            gcs_bucket_obj (google.cloud.storage.Bucket): Initialized GCS bucket object.\n",
        "            lookup_table (np.array): Numpy array for remapping mask labels.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.pairs = image_mask_pairs\n",
        "        self.bucket = gcs_bucket_obj\n",
        "        self.lookup_table = lookup_table\n",
        "        self.transform = transform\n",
        "        self.imagenet_mean = np.array([0.485, 0.456, 0.406]) # Store for dummy data if needed\n",
        "        self.imagenet_std = np.array([0.229, 0.224, 0.225]) # Store for dummy data if needed\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return 0 if pairs list is None or empty\n",
        "        return len(self.pairs) if self.pairs else 0\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # --- Robust handling for empty list or invalid index ---\n",
        "        dataset_len = len(self.pairs) if self.pairs else 0\n",
        "        # *** CORRECTED CHECK: Removed isinstance(idx, int) ***\n",
        "        if dataset_len == 0 or idx < 0 or idx >= dataset_len:\n",
        "             print(f\"Warning: Invalid index {idx} for dataset length {dataset_len}. Attempting fallback.\")\n",
        "             # If invalid, try returning the first item if possible\n",
        "             if dataset_len > 0:\n",
        "                 idx = 0\n",
        "                 print(\"Falling back to index 0.\")\n",
        "             else:\n",
        "                 print(\"Error: Dataset is empty, cannot fallback.\")\n",
        "                 # Create and return dummy data if dataset is truly empty\n",
        "                 dummy_image_np = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\n",
        "                 dummy_mask_np = np.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
        "                 image_tensor = torch.zeros((3, IMG_HEIGHT, IMG_WIDTH), dtype=torch.float32)\n",
        "                 mask_tensor = torch.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=torch.long)\n",
        "                 # Apply dummy transform\n",
        "                 if self.transform:\n",
        "                      dummy_transform = A.Compose([ A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, interpolation=cv2.INTER_NEAREST), A.Normalize(mean=self.imagenet_mean, std=self.imagenet_std), ToTensorV2() ])\n",
        "                      try:\n",
        "                           augmented = dummy_transform(image=dummy_image_np, mask=dummy_mask_np)\n",
        "                           image_tensor, mask_tensor = augmented['image'], augmented['mask'].long()\n",
        "                      except: pass\n",
        "                 return {\"image\": image_tensor, \"mask\": mask_tensor}\n",
        "        # --- End robust handling ---\n",
        "\n",
        "\n",
        "        try:\n",
        "             img_path, mask_path = self.pairs[idx]\n",
        "        except IndexError:\n",
        "             print(f\"Error: Caught IndexError accessing self.pairs[{idx}] despite checks. Len={dataset_len}\")\n",
        "             # Fallback to dummy data if error occurs even after checks\n",
        "             dummy_image_np = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\n",
        "             dummy_mask_np = np.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
        "             image_tensor = torch.zeros((3, IMG_HEIGHT, IMG_WIDTH), dtype=torch.float32)\n",
        "             mask_tensor = torch.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=torch.long)\n",
        "             if self.transform:\n",
        "                  dummy_transform = A.Compose([ A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, interpolation=cv2.INTER_NEAREST), A.Normalize(mean=self.imagenet_mean, std=self.imagenet_std), ToTensorV2() ])\n",
        "                  try:\n",
        "                       augmented = dummy_transform(image=dummy_image_np, mask=dummy_mask_np)\n",
        "                       image_tensor, mask_tensor = augmented['image'], augmented['mask'].long()\n",
        "                  except: pass\n",
        "             return {\"image\": image_tensor, \"mask\": mask_tensor}\n",
        "\n",
        "\n",
        "        # Load image (returns PIL Image, converted to RGB)\n",
        "        image = load_image_from_gcs(img_path, self.bucket)\n",
        "        # Load mask (returns numpy array, single channel L)\n",
        "        mask = load_mask_from_gcs(mask_path, self.bucket)\n",
        "\n",
        "        # Handle potential loading errors (This section might now be less likely to hit the 'next sample' logic due to __getitem__ fixes)\n",
        "        if image is None or mask is None:\n",
        "            print(f\"Warning: Failed to load image or mask for index {idx}. Paths: {img_path}, {mask_path}.\")\n",
        "            # Return dummy data for this specific failure\n",
        "            dummy_image_np = np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8)\n",
        "            dummy_mask_np = np.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=np.uint8)\n",
        "            image_tensor = torch.zeros((3, IMG_HEIGHT, IMG_WIDTH), dtype=torch.float32)\n",
        "            mask_tensor = torch.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=torch.long)\n",
        "            if self.transform:\n",
        "                dummy_transform = A.Compose([ A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, interpolation=cv2.INTER_NEAREST), A.Normalize(mean=self.imagenet_mean, std=self.imagenet_std), ToTensorV2() ])\n",
        "                try:\n",
        "                    augmented = dummy_transform(image=dummy_image_np, mask=dummy_mask_np)\n",
        "                    image_tensor, mask_tensor = augmented['image'], augmented['mask'].long()\n",
        "                except: pass\n",
        "            return {\"image\": image_tensor, \"mask\": mask_tensor}\n",
        "\n",
        "\n",
        "        # Remap mask labels\n",
        "        if not mask.flags.writeable:\n",
        "             mask = mask.copy()\n",
        "        mask_remapped = remap_mask_labels(mask, self.lookup_table)\n",
        "\n",
        "        # Convert PIL image to numpy array for albumentations\n",
        "        image_np = np.array(image)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            try:\n",
        "                augmented = self.transform(image=image_np, mask=mask_remapped)\n",
        "                image_tensor = augmented['image']\n",
        "                mask_tensor = augmented['mask'] # Albumentations ToTensorV2 handles mask type\n",
        "            except Exception as e:\n",
        "                 print(f\"Error during augmentation for index {idx}: {e}. Paths: {img_path}, {mask_path}\")\n",
        "                 # Fallback to basic transform or dummy data? Let's try basic.\n",
        "                 basic_transform = A.Compose([\n",
        "                     A.Resize(height=IMG_HEIGHT, width=IMG_WIDTH, interpolation=cv2.INTER_NEAREST),\n",
        "                     A.Normalize(mean=self.imagenet_mean, std=self.imagenet_std),\n",
        "                     ToTensorV2(),\n",
        "                 ])\n",
        "                 try:\n",
        "                      augmented = basic_transform(image=image_np, mask=mask_remapped)\n",
        "                      image_tensor = augmented['image']\n",
        "                      mask_tensor = augmented['mask']\n",
        "                 except Exception as basic_e:\n",
        "                      print(f\"Fallback basic transform failed: {basic_e}. Returning dummy data.\")\n",
        "                      image_tensor = torch.zeros((3, IMG_HEIGHT, IMG_WIDTH), dtype=torch.float32)\n",
        "                      mask_tensor = torch.zeros((IMG_HEIGHT, IMG_WIDTH), dtype=torch.long)\n",
        "\n",
        "        else:\n",
        "            # Basic conversion if no transforms (should not happen with setup)\n",
        "            image_tensor = torch.from_numpy(image_np).permute(2, 0, 1).float() / 255.0\n",
        "            mask_tensor = torch.from_numpy(mask_remapped)\n",
        "\n",
        "        # Ensure mask is LongTensor for CrossEntropyLoss\n",
        "        mask_tensor = mask_tensor.long()\n",
        "\n",
        "        return {\"image\": image_tensor, \"mask\": mask_tensor}\n",
        "\n",
        "\n",
        "# --- Create Datasets ---\n",
        "# (Code remains the same as the version with the test set)\n",
        "train_dataset = BatteryComponentDataset(train_pairs, gcs_bucket, lookup_table, transform=train_transform) if train_pairs else None\n",
        "val_dataset = BatteryComponentDataset(val_pairs, gcs_bucket, lookup_table, transform=val_transform) if val_pairs else None\n",
        "test_dataset = BatteryComponentDataset(test_pairs, gcs_bucket, lookup_table, transform=val_transform) if test_pairs else None\n",
        "\n",
        "print(\"\\nDatasets created.\")\n",
        "print(f\"Train dataset size: {len(train_dataset) if train_dataset else 0}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset) if val_dataset else 0}\")\n",
        "print(f\"Test dataset size: {len(test_dataset) if test_dataset else 0}\")\n",
        "\n",
        "\n",
        "# --- Create DataLoaders ---\n",
        "# (Code remains the same as the version with the test set)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True if train_dataset and len(train_dataset) > BATCH_SIZE else False, drop_last=True) if train_dataset and len(train_dataset) > 0 else None\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True if val_dataset and len(val_dataset) > BATCH_SIZE else False) if val_dataset and len(val_dataset) > 0 else None\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True if test_dataset and len(test_dataset) > 0 else False) if test_dataset and len(test_dataset) > 0 else None\n",
        "\n",
        "print(\"\\nDataLoaders created.\")\n",
        "print(f\"Number of training batches: {len(train_loader) if train_loader else 0}\")\n",
        "print(f\"Number of validation batches: {len(val_loader) if val_loader else 0}\")\n",
        "print(f\"Number of test batches: {len(test_loader) if test_loader else 0}\")\n",
        "\n",
        "\n",
        "# --- Optional: Visualize a sample from the DataLoader to verify ---\n",
        "# (Visualization code remains the same)\n",
        "print(\"\\nVisualizing one sample from training loader (if available)...\")\n",
        "# ... (Keep the visualization block as it was) ...\n",
        "if train_loader:\n",
        "    try:\n",
        "        # Make sure dataset is not empty before trying iter\n",
        "        if len(train_loader.dataset) > 0:\n",
        "             sample_batch = next(iter(train_loader))\n",
        "             img_sample = sample_batch['image'][0]\n",
        "             mask_sample = sample_batch['mask'][0]\n",
        "\n",
        "             print(f\"Image batch shape: {sample_batch['image'].shape}\")\n",
        "             print(f\"Mask batch shape: {sample_batch['mask'].shape}\")\n",
        "             print(f\"Image sample shape: {img_sample.shape}\")\n",
        "             print(f\"Mask sample shape: {mask_sample.shape}\")\n",
        "             print(f\"Image sample dtype: {img_sample.dtype}\")\n",
        "             print(f\"Mask sample dtype: {mask_sample.dtype}\")\n",
        "             print(f\"Unique values in mask sample: {torch.unique(mask_sample)}\")\n",
        "\n",
        "             # Reverse normalization for display\n",
        "             img_display = img_sample.permute(1, 2, 0).cpu().numpy() # CHW -> HWC\n",
        "             mean = np.array(imagenet_mean)\n",
        "             std = np.array(imagenet_std)\n",
        "             img_display = std * img_display + mean\n",
        "             img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "             mask_display = mask_sample.cpu().numpy()\n",
        "\n",
        "             fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "             ax[0].imshow(img_display)\n",
        "             ax[0].set_title(\"Sample Image (Augmented)\")\n",
        "             ax[0].axis('off')\n",
        "             # Use a suitable colormap for masks, ensure vmin/vmax cover your classes\n",
        "             im = ax[1].imshow(mask_display, cmap='tab10', vmin=0, vmax=NUM_CLASSES-1)\n",
        "             ax[1].set_title(\"Sample Mask (Remapped & Augmented)\")\n",
        "             ax[1].axis('off')\n",
        "             # Add colorbar to understand mask labels\n",
        "             cbar = fig.colorbar(im, ax=ax[1], ticks=np.arange(NUM_CLASSES))\n",
        "             cbar.ax.set_yticklabels([f'Class {i}' for i in range(NUM_CLASSES)]) # Label classes\n",
        "             plt.show()\n",
        "        else:\n",
        "             print(\"Training dataset is empty, cannot visualize sample.\")\n",
        "\n",
        "    except StopIteration:\n",
        "         print(\"Error: Training DataLoader is empty or could not fetch a batch.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error visualizing sample: {e}\")\n",
        "        print(\"Check dataset loading and transformations.\")\n",
        "else:\n",
        "     print(\"Training DataLoader not created (dataset likely empty). Skipping visualization.\")"
      ],
      "metadata": {
        "id": "bM5dbOOBrnXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 16: Define Model, Loss, Optimizer, Metrics\n",
        "\n",
        "# --- Load Model ---\n",
        "# (Model loading code remains the same)\n",
        "print(f\"Loading model: {MODEL_ARCH}\")\n",
        "if MODEL_ARCH == 'deeplabv3_resnet50':\n",
        "    weights = DeepLabV3_ResNet50_Weights.DEFAULT # Use default weights (COCO pre-trained)\n",
        "    model = deeplabv3_resnet50(weights=weights)\n",
        "    # Modify the final classifier layer for our number of classes\n",
        "    # The classifier head in deeplabv3 is model.classifier[-1]\n",
        "    model.classifier[-1] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "    # Also modify auxiliary classifier if it exists (depends on weights version)\n",
        "    if hasattr(model, 'aux_classifier') and model.aux_classifier is not None:\n",
        "         model.aux_classifier[-1] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "\n",
        "elif MODEL_ARCH == 'fcn_resnet50':\n",
        "    weights = FCN_ResNet50_Weights.DEFAULT\n",
        "    model = fcn_resnet50(weights=weights)\n",
        "    model.classifier[-1] = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "    if hasattr(model, 'aux_classifier') and model.aux_classifier is not None:\n",
        "        model.aux_classifier[-1] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported MODEL_ARCH: {MODEL_ARCH}\")\n",
        "\n",
        "model = model.to(device)\n",
        "print(f\"Model loaded and moved to {device}.\")\n",
        "\n",
        "# --- Loss Function ---\n",
        "# (Class weighting logic remains the same)\n",
        "class_weights = None\n",
        "if USE_CLASS_WEIGHTING:\n",
        "    print(\"Calculating class weights (may take a while)...\")\n",
        "    if train_loader is None or len(train_loader.dataset) == 0:\n",
        "        print(\"Warning: Training data unavailable, cannot calculate class weights.\")\n",
        "    else:\n",
        "        label_counts = np.zeros(NUM_CLASSES)\n",
        "        num_samples_for_weights = min(len(train_dataset), 5000)\n",
        "        print(f\"Calculating weights based on approx {num_samples_for_weights} training samples...\")\n",
        "        # Create a temporary loader for weight calculation if main loader has shuffle=True\n",
        "        temp_loader_weights = DataLoader(train_dataset, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=1)\n",
        "\n",
        "        processed_samples = 0\n",
        "        for batch in tqdm(temp_loader_weights, desc=\"Counting labels for weights\"):\n",
        "            masks = batch['mask'].numpy() # (B, H, W)\n",
        "            unique, counts = np.unique(masks, return_counts=True)\n",
        "            for label, count in zip(unique, counts):\n",
        "                if 0 <= label < NUM_CLASSES:\n",
        "                    label_counts[label] += count\n",
        "            processed_samples += masks.shape[0]\n",
        "            if processed_samples >= num_samples_for_weights:\n",
        "                 break\n",
        "        del temp_loader_weights # Free memory\n",
        "\n",
        "        print(f\"Raw label counts (from ~{processed_samples} samples): {label_counts}\")\n",
        "        label_counts = np.maximum(label_counts, 1) # Avoid division by zero\n",
        "        total_pixels = label_counts.sum()\n",
        "        class_weights_inv = total_pixels / (NUM_CLASSES * label_counts)\n",
        "        # class_weights_inv /= np.mean(class_weights_inv) # Normalize by mean\n",
        "        class_weights_inv /= class_weights_inv.sum() # Normalize weights to sum to 1\n",
        "\n",
        "        class_weights = torch.tensor(class_weights_inv, dtype=torch.float).to(device)\n",
        "        print(\"Class weights calculated (normalized):\", class_weights)\n",
        "else:\n",
        "     print(\"Class weighting disabled.\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights) # Pass None if not using weights\n",
        "print(f\"Loss function: CrossEntropyLoss (Weights: {'Enabled' if class_weights is not None else 'Disabled'})\")\n",
        "\n",
        "\n",
        "# --- Optimizer ---\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=OPTIMIZER_WEIGHT_DECAY)\n",
        "print(f\"Optimizer: AdamW (LR={LEARNING_RATE}, WD={OPTIMIZER_WEIGHT_DECAY})\")\n",
        "\n",
        "# --- Learning Rate Scheduler (Optional) ---\n",
        "scheduler = None\n",
        "if USE_LR_SCHEDULER:\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=SCHEDULER_FACTOR, patience=SCHEDULER_PATIENCE, verbose=True)\n",
        "    print(f\"LR Scheduler: ReduceLROnPlateau (Mode=max, Monitor=val_mIoU, Patience={SCHEDULER_PATIENCE}, Factor={SCHEDULER_FACTOR})\")\n",
        "\n",
        "\n",
        "# --- Metrics (using torchmetrics) ---\n",
        "# Base metrics\n",
        "jaccard_macro_base = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average='macro', ignore_index=None).to(device) # mIoU\n",
        "jaccard_none_base = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=NUM_CLASSES, average='none', ignore_index=None).to(device) # Per-class IoU\n",
        "accuracy_macro_base = torchmetrics.Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES, average='macro').to(device) # Mean class accuracy\n",
        "\n",
        "# Metric collections for each phase (cloning ensures independent states)\n",
        "metric_collection_train = torchmetrics.MetricCollection({\n",
        "    'train_mIoU': jaccard_macro_base.clone(),\n",
        "    'train_pixelAcc': accuracy_macro_base.clone()\n",
        "})\n",
        "metric_collection_val = torchmetrics.MetricCollection({\n",
        "    'val_mIoU': jaccard_macro_base.clone(),\n",
        "    'val_pixelAcc': accuracy_macro_base.clone()\n",
        "})\n",
        "# For test set - reuse validation collection structure, will be reset before use\n",
        "metric_collection_test = metric_collection_val.clone()\n",
        "# For per-class IoU during validation and test\n",
        "per_class_iou_metric = jaccard_none_base.clone()\n",
        "\n",
        "\n",
        "print(\"Metrics initialized (mIoU, Pixel Accuracy) for train, validation, and test.\")"
      ],
      "metadata": {
        "id": "hHgX7pYsr4c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 17: Training and Validation Loop (with Early Stopping)\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, device, epoch_num, metric_collection_train):\n",
        "    # (Function definition remains the same)\n",
        "    model.train()\n",
        "    metric_collection_train.reset()\n",
        "    running_loss = 0.0\n",
        "    num_samples = 0\n",
        "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch_num+1}/{NUM_EPOCHS} [Train]\", leave=False)\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        if batch is None or 'image' not in batch or 'mask' not in batch: continue\n",
        "        images = batch['image'].to(device)\n",
        "        masks = batch['mask'].to(device)\n",
        "        if images.shape[0] == 0: continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        try:\n",
        "            outputs = model(images)\n",
        "            main_output = outputs['out']\n",
        "            loss = criterion(main_output, masks)\n",
        "            if 'aux' in outputs and outputs['aux'] is not None:\n",
        "                aux_loss = criterion(outputs['aux'], masks)\n",
        "                loss = loss + 0.4 * aux_loss\n",
        "        except Exception as e:\n",
        "             print(f\"\\nError during forward/loss calculation in training batch {batch_idx}: {e}\")\n",
        "             continue\n",
        "\n",
        "        try:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        except Exception as e:\n",
        "             print(f\"\\nError during backward/optimizer step in training batch {batch_idx}: {e}\")\n",
        "             continue\n",
        "\n",
        "        preds = torch.argmax(main_output.detach(), dim=1)\n",
        "        metric_collection_train.update(preds, masks)\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        num_samples += images.size(0)\n",
        "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    if num_samples == 0:\n",
        "         print(f\"Epoch {epoch_num+1} Train: No samples processed.\")\n",
        "         return 0.0, {}\n",
        "    epoch_loss = running_loss / num_samples\n",
        "    epoch_metrics = metric_collection_train.compute()\n",
        "    print(f\"Epoch {epoch_num+1} Train Loss: {epoch_loss:.4f}\", end=\" | \")\n",
        "    for name, value in epoch_metrics.items():\n",
        "        print(f\"{name}: {value:.4f}\", end=\" \")\n",
        "    print()\n",
        "    return epoch_loss, epoch_metrics\n",
        "\n",
        "\n",
        "def validate_one_epoch(model, loader, criterion, device, epoch_num, metric_collection_val, per_class_metric):\n",
        "    # (Function definition remains the same)\n",
        "    model.eval()\n",
        "    metric_collection_val.reset()\n",
        "    per_class_metric.reset()\n",
        "    running_loss = 0.0\n",
        "    num_samples = 0\n",
        "    progress_bar = tqdm(loader, desc=f\"Epoch {epoch_num+1}/{NUM_EPOCHS} [Val]\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            if batch is None or 'image' not in batch or 'mask' not in batch: continue\n",
        "            images = batch['image'].to(device)\n",
        "            masks = batch['mask'].to(device)\n",
        "            if images.shape[0] == 0: continue\n",
        "\n",
        "            try:\n",
        "                outputs = model(images)\n",
        "                main_output = outputs['out']\n",
        "                loss = criterion(main_output, masks)\n",
        "            except Exception as e:\n",
        "                 print(f\"\\nError during forward/loss calculation in validation batch {batch_idx}: {e}\")\n",
        "                 continue\n",
        "\n",
        "            preds = torch.argmax(main_output.detach(), dim=1)\n",
        "            metric_collection_val.update(preds, masks)\n",
        "            per_class_metric.update(preds, masks)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            num_samples += images.size(0)\n",
        "            progress_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    if num_samples == 0:\n",
        "         print(f\"Epoch {epoch_num+1} Val: No samples processed.\")\n",
        "         return 0.0, {}, torch.zeros(NUM_CLASSES)\n",
        "    epoch_loss = running_loss / num_samples\n",
        "    epoch_metrics = metric_collection_val.compute()\n",
        "    per_class_iou = per_class_metric.compute()\n",
        "\n",
        "    print(f\"Epoch {epoch_num+1} Val Loss: {epoch_loss:.4f}\", end=\" | \")\n",
        "    for name, value in epoch_metrics.items():\n",
        "         print(f\"{name}: {value:.4f}\", end=\" \")\n",
        "    print()\n",
        "    class_names = [\"bg\", \"bolt\", \"busbar\", \"cable\", \"connector\", \"nut\", \"film\", \"cover\"]\n",
        "    print(\"  Per-Class IoU:\", end=\" \")\n",
        "    for i, iou in enumerate(per_class_iou):\n",
        "         class_name = class_names[i] if i < len(class_names) else f\"C{i}\"\n",
        "         print(f\"{class_name}: {iou:.3f}\", end=\" \")\n",
        "    print()\n",
        "    return epoch_loss, epoch_metrics, per_class_iou\n",
        "\n",
        "\n",
        "# --- Training Loop ---\n",
        "best_val_miou = -1.0\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_mIoU': [], 'val_pixelAcc': []}\n",
        "epochs_completed = 0 # Keep track of actual epochs run\n",
        "\n",
        "# --- Early Stopping Initialization ---\n",
        "if USE_EARLY_STOPPING:\n",
        "    epochs_no_improve = 0\n",
        "    best_val_loss_for_stopping = float('inf')\n",
        "    print(f\"Early stopping enabled: Patience={EARLY_STOPPING_PATIENCE}, Min Delta={EARLY_STOPPING_MIN_DELTA}\")\n",
        "# ------------------------------------\n",
        "\n",
        "# Check if loaders exist before starting training\n",
        "if not train_loader or not val_loader:\n",
        "     print(\"\\nError: Training or Validation DataLoader is not available. Check dataset splits and sizes.\")\n",
        "     print(\"Skipping training loop.\")\n",
        "else:\n",
        "    print(\"\\n--- Starting Training ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epochs_completed = epoch + 1 # Track actual epochs run\n",
        "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        train_loss, train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, metric_collection_train)\n",
        "        val_loss, val_metrics, _ = validate_one_epoch(model, val_loader, criterion, device, epoch, metric_collection_val, per_class_iou_metric) # Pass per-class metric here\n",
        "\n",
        "        # Store history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_mIoU'].append(val_metrics.get('val_mIoU', torch.tensor(0.0)).item())\n",
        "        history['val_pixelAcc'].append(val_metrics.get('val_pixelAcc', torch.tensor(0.0)).item())\n",
        "\n",
        "        current_val_miou = val_metrics.get('val_mIoU', torch.tensor(-1.0))\n",
        "\n",
        "        # Learning rate scheduling step (based on validation mIoU)\n",
        "        if USE_LR_SCHEDULER and scheduler:\n",
        "            scheduler.step(current_val_miou)\n",
        "\n",
        "        # Save the best model based on validation mIoU (primary performance metric)\n",
        "        if current_val_miou > best_val_miou:\n",
        "            print(f\"Validation mIoU improved ({best_val_miou:.4f} --> {current_val_miou:.4f}). Saving model...\")\n",
        "            best_val_miou = current_val_miou\n",
        "            try:\n",
        "                torch.save(model.state_dict(), CHECKPOINT_PATH)\n",
        "                print(f\"*** Best model saved to {CHECKPOINT_PATH} ***\")\n",
        "            except Exception as save_e:\n",
        "                 print(f\"!!! Error saving model checkpoint: {save_e} !!!\")\n",
        "\n",
        "        # --- Early Stopping Check (based on validation loss) ---\n",
        "        if USE_EARLY_STOPPING:\n",
        "            # Check if val_loss improved significantly\n",
        "            if val_loss < best_val_loss_for_stopping - EARLY_STOPPING_MIN_DELTA:\n",
        "                best_val_loss_for_stopping = val_loss\n",
        "                epochs_no_improve = 0\n",
        "                print(f\"Val loss improved to {val_loss:.4f}. Resetting early stopping counter.\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                print(f\"Val loss ({val_loss:.4f}) did not improve significantly from best ({best_val_loss_for_stopping:.4f}). Counter: {epochs_no_improve}/{EARLY_STOPPING_PATIENCE}\")\n",
        "\n",
        "            if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"\\n--- Early Stopping Triggered ---\")\n",
        "                print(f\"Validation loss did not improve for {EARLY_STOPPING_PATIENCE} consecutive epochs.\")\n",
        "                break # Exit the training loop\n",
        "        # ----------------------------------------------------\n",
        "\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    print(f\"--- Training Finished ---\")\n",
        "    print(f\"Epochs Run: {epochs_completed}\") # Report actual epochs run\n",
        "    print(f\"Total Training Time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
        "    print(f\"Best Validation mIoU achieved: {best_val_miou:.4f} (at epoch saved)\")\n",
        "\n",
        "\n",
        "    # --- Plot Training History ---\n",
        "    if epochs_completed > 0: # Use actual epochs completed for plotting\n",
        "        epochs_range_actual = range(1, epochs_completed + 1) # Range based on actual epochs run\n",
        "        plt.figure(figsize=(14, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs_range_actual, history['train_loss'], 'bo-', label='Training Loss')\n",
        "        plt.plot(epochs_range_actual, history['val_loss'], 'ro-', label='Validation Loss')\n",
        "        plt.legend(loc='upper right')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs_range_actual, history['val_mIoU'], 'go-', label='Validation mIoU')\n",
        "        plt.plot(epochs_range_actual, history['val_pixelAcc'], 'mo-', label='Validation Mean Pixel Accuracy')\n",
        "        plt.legend(loc='lower right')\n",
        "        plt.title('Validation Metrics')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Metric Value')\n",
        "        plt.ylim([0, 1]) # Metrics are typically between 0 and 1\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No epochs were run, skipping history plot.\")"
      ],
      "metadata": {
        "id": "BWHRmB_tsCOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 17b: Evaluate Best Model on Test Set (Corrected Model Loading)\n",
        "\n",
        "print(\"\\n--- Evaluating Best Model on Test Set ---\")\n",
        "\n",
        "# Check if a checkpoint exists and the test loader is valid\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "     print(f\"Checkpoint file not found at {CHECKPOINT_PATH}. Cannot evaluate on test set.\")\n",
        "elif test_loader is None or len(test_loader.dataset) == 0:\n",
        "     print(\"Test loader is empty or not created. Cannot evaluate on test set.\")\n",
        "else:\n",
        "    model_test = None # Initialize to None\n",
        "    # --- Corrected Model Re-initialization ---\n",
        "    # Re-initialize model structure *matching the training setup*\n",
        "    # to ensure aux_classifier exists if it was saved in the state_dict.\n",
        "    print(f\"Re-initializing model structure: {MODEL_ARCH}\")\n",
        "    if MODEL_ARCH == 'deeplabv3_resnet50':\n",
        "        # Initialize with default weights to ensure aux classifier exists if expected\n",
        "        model_test = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT) # <<< FIX: Use default weights initially\n",
        "        # Now modify heads to match NUM_CLASSES *before* loading state_dict\n",
        "        model_test.classifier[-1] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "        # The aux classifier should exist now if it did during training\n",
        "        if hasattr(model_test, 'aux_classifier') and model_test.aux_classifier is not None:\n",
        "            model_test.aux_classifier[-1] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "            print(\"Auxiliary classifier head modified.\")\n",
        "        else:\n",
        "             # This case shouldn't happen if training model had aux head, but good check\n",
        "             print(\"Warning: Re-initialized model for testing lacks aux_classifier, potential mismatch with saved state_dict if it contained aux keys.\")\n",
        "\n",
        "    elif MODEL_ARCH == 'fcn_resnet50':\n",
        "         # Apply similar logic for FCN if you used that\n",
        "         model_test = fcn_resnet50(weights=FCN_ResNet50_Weights.DEFAULT) # <<< FIX: Use default weights initially\n",
        "         model_test.classifier[-1] = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "         if hasattr(model_test, 'aux_classifier') and model_test.aux_classifier is not None:\n",
        "            model_test.aux_classifier[-1] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "            print(\"Auxiliary classifier head modified.\")\n",
        "         else:\n",
        "              print(\"Warning: Re-initialized FCN model for testing lacks aux_classifier.\")\n",
        "    else:\n",
        "        print(f\"ERROR: MODEL_ARCH {MODEL_ARCH} not recognized during test reloading.\")\n",
        "        model_test = None\n",
        "    # --- End Model Re-initialization ---\n",
        "\n",
        "\n",
        "    # --- Load State Dict and Evaluate ---\n",
        "    if model_test is not None:\n",
        "        try:\n",
        "            print(f\"Loading best model weights from {CHECKPOINT_PATH} for test evaluation...\")\n",
        "            map_location = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            # Load the saved weights into the correctly structured model\n",
        "            model_test.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=map_location))\n",
        "            model_test.to(device)\n",
        "            model_test.eval() # Set to evaluation mode\n",
        "            print(\"Model loaded successfully.\")\n",
        "\n",
        "            # Reset metric collections before test evaluation\n",
        "            metric_collection_test.reset() # Use the dedicated test collection\n",
        "            per_class_iou_metric.reset() # Reset per-class metric too\n",
        "\n",
        "            test_loss = 0.0 # Can optionally calculate test loss too\n",
        "            num_test_samples = 0\n",
        "            progress_bar_test = tqdm(test_loader, desc=\"Testing\", leave=False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_idx, batch in enumerate(progress_bar_test):\n",
        "                    # Added checks for invalid batch data\n",
        "                    if batch is None or 'image' not in batch or 'mask' not in batch:\n",
        "                         print(f\"Skipping invalid batch {batch_idx} in testing\")\n",
        "                         continue\n",
        "                    images = batch['image'].to(device)\n",
        "                    masks = batch['mask'].to(device)\n",
        "\n",
        "                    if images.shape[0] == 0: continue # Skip if batch size is 0\n",
        "\n",
        "                    try:\n",
        "                        outputs = model_test(images)\n",
        "                        main_output = outputs['out']\n",
        "\n",
        "                        # Optional: Calculate loss on test set\n",
        "                        loss = criterion(main_output, masks) # Use the same criterion\n",
        "                        test_loss += loss.item() * images.size(0)\n",
        "                        num_test_samples += images.size(0)\n",
        "\n",
        "                        # Update metrics\n",
        "                        preds = torch.argmax(main_output.detach(), dim=1)\n",
        "                        metric_collection_test.update(preds, masks)\n",
        "                        per_class_iou_metric.update(preds, masks)\n",
        "\n",
        "                    except Exception as e:\n",
        "                         print(f\"\\nError during testing batch {batch_idx}: {e}\")\n",
        "                         print(f\"Image shape: {images.shape}, Mask shape: {masks.shape}\")\n",
        "                         continue # Skip batch on error\n",
        "\n",
        "            # Compute final test metrics\n",
        "            if num_test_samples > 0:\n",
        "                 final_test_loss = test_loss / num_test_samples\n",
        "                 # Compute metrics from the dedicated test collection\n",
        "                 final_test_metrics = metric_collection_test.compute()\n",
        "                 final_per_class_iou = per_class_iou_metric.compute()\n",
        "\n",
        "                 print(\"\\n--- Test Set Results ---\")\n",
        "                 print(f\"Test Loss: {final_test_loss:.4f}\")\n",
        "                 # Fetch metrics using their keys in the test collection ('val_...' because cloned from val collection)\n",
        "                 print(f\"Test mIoU: {final_test_metrics.get('val_mIoU', torch.tensor(-1.0)):.4f}\")\n",
        "                 print(f\"Test Mean Pixel Accuracy: {final_test_metrics.get('val_pixelAcc', torch.tensor(-1.0)):.4f}\")\n",
        "\n",
        "                 # Print per-class IoU\n",
        "                 class_names = [\"bg\", \"bolt\", \"busbar\", \"cable\", \"connector\", \"nut\", \"film\", \"cover\"] # Short names\n",
        "                 print(\"  Per-Class IoU:\", end=\" \")\n",
        "                 for i, iou in enumerate(final_per_class_iou):\n",
        "                     class_name = class_names[i] if i < len(class_names) else f\"C{i}\"\n",
        "                     print(f\"{class_name}: {iou:.3f}\", end=\" \")\n",
        "                 print()\n",
        "                 print(\"-------------------------\")\n",
        "            else:\n",
        "                 print(\"No samples processed in the test set evaluation.\")\n",
        "\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Error: Checkpoint file not found at {CHECKPOINT_PATH}. Cannot evaluate test set.\")\n",
        "        except Exception as e:\n",
        "            # Catch potential errors during model loading or evaluation loop\n",
        "            print(f\"Error during test set evaluation process: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc() # Print detailed traceback for debugging\n",
        "    else:\n",
        "         print(\"Test model initialization failed. Cannot evaluate.\")\n",
        "# --- End Evaluation Block ---"
      ],
      "metadata": {
        "id": "bdrAj7QJaE7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 18: Load Best Model and Evaluate (Optional: Example Inference on Test Set - Corrected)\n",
        "\n",
        "# --- Visualization Setup ---\n",
        "num_inference_samples = 5 # How many samples to visualize\n",
        "\n",
        "# Check if test dataset and pairs exist before setting them for inference\n",
        "if test_dataset and test_pairs:\n",
        "    inference_dataset = test_dataset\n",
        "    inference_pairs = test_pairs\n",
        "    inference_set_name = \"Test\"\n",
        "    print(\"Setting inference source to Test Set.\")\n",
        "elif val_dataset and val_pairs: # Fallback to validation set if test set is unavailable\n",
        "     inference_dataset = val_dataset\n",
        "     inference_pairs = val_pairs\n",
        "     inference_set_name = \"Validation\"\n",
        "     print(\"Warning: Test set not available, setting inference source to Validation Set.\")\n",
        "else:\n",
        "    inference_dataset = None\n",
        "    inference_pairs = None\n",
        "    inference_set_name = \"N/A\"\n",
        "    print(\"Warning: No Test or Validation set available for inference.\")\n",
        "\n",
        "\n",
        "# --- Load the best saved model weights ---\n",
        "best_model = None # Initialize best_model\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    print(f\"Loading best model weights from {CHECKPOINT_PATH}\")\n",
        "\n",
        "    # --- Corrected Model Re-initialization for Inference ---\n",
        "    # Re-initialize model structure *matching the training setup*\n",
        "    print(f\"Re-initializing model structure for inference: {MODEL_ARCH}\")\n",
        "    if MODEL_ARCH == 'deeplabv3_resnet50':\n",
        "        # Initialize with default weights to ensure aux classifier exists if expected\n",
        "        best_model = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT) # <<< FIX: Use default weights\n",
        "        # Now modify heads to match NUM_CLASSES *before* loading state_dict\n",
        "        best_model.classifier[-1] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "        if hasattr(best_model, 'aux_classifier') and best_model.aux_classifier is not None:\n",
        "            best_model.aux_classifier[-1] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "            print(\"Auxiliary classifier head modified for inference model.\")\n",
        "        else:\n",
        "             print(\"Warning: Re-initialized inference model lacks aux_classifier.\")\n",
        "\n",
        "    elif MODEL_ARCH == 'fcn_resnet50':\n",
        "         best_model = fcn_resnet50(weights=FCN_ResNet50_Weights.DEFAULT) # <<< FIX: Use default weights\n",
        "         best_model.classifier[-1] = nn.Conv2d(512, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "         if hasattr(best_model, 'aux_classifier') and best_model.aux_classifier is not None:\n",
        "            best_model.aux_classifier[-1] = nn.Conv2d(256, NUM_CLASSES, kernel_size=(1, 1), stride=(1, 1))\n",
        "            print(\"Auxiliary classifier head modified for inference model.\")\n",
        "         else:\n",
        "              print(\"Warning: Re-initialized FCN inference model lacks aux_classifier.\")\n",
        "    else:\n",
        "        print(f\"ERROR: MODEL_ARCH {MODEL_ARCH} not recognized during inference reloading.\")\n",
        "        best_model = None\n",
        "    # --- End Model Re-initialization ---\n",
        "\n",
        "    # --- Load State Dict ---\n",
        "    if best_model is not None:\n",
        "        try:\n",
        "            map_location = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            print(f\"Loading weights onto device: {map_location}\")\n",
        "            # Load the saved weights into the correctly structured model\n",
        "            best_model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=map_location))\n",
        "            best_model.to(device)\n",
        "            best_model.eval() # Set to evaluation mode\n",
        "            print(\"Best model loaded successfully for inference.\")\n",
        "        except FileNotFoundError: # Should be caught by os.path.exists, but defensive check\n",
        "            print(f\"Error: Checkpoint file disappeared at {CHECKPOINT_PATH}.\")\n",
        "            best_model = None\n",
        "        except Exception as e:\n",
        "            # Catch errors during loading (like the unexpected keys error)\n",
        "            print(f\"Error loading model state dict for inference: {e}\")\n",
        "            best_model = None # Set best_model to None if loading fails\n",
        "else:\n",
        "     print(f\"Checkpoint file not found at {CHECKPOINT_PATH}. Cannot run inference.\")\n",
        "# --- End Loading Block ---\n",
        "\n",
        "\n",
        "# --- Example Inference on a few Samples from the chosen dataset ---\n",
        "if best_model and inference_dataset and len(inference_dataset) > 0:\n",
        "    print(f\"\\nRunning inference on {num_inference_samples} samples from the {inference_set_name} set...\")\n",
        "    # Ensure we don't request more samples than available\n",
        "    num_samples_to_show = min(num_inference_samples, len(inference_dataset))\n",
        "    indices_to_show = np.random.choice(range(len(inference_dataset)), num_samples_to_show, replace=False)\n",
        "\n",
        "    for sample_idx in indices_to_show:\n",
        "        print(f\"\\n--- Sample Index in {inference_set_name} Set: {sample_idx} ---\")\n",
        "        try:\n",
        "             sample = inference_dataset[sample_idx] # Get sample\n",
        "        except Exception as getitem_e:\n",
        "             print(f\"Error getting sample {sample_idx} from dataset: {getitem_e}\")\n",
        "             continue # Skip this sample\n",
        "\n",
        "        # Handle potential issues if dataset returns None or invalid structure\n",
        "        if sample is None or 'image' not in sample or 'mask' not in sample:\n",
        "             print(f\"Skipping sample {sample_idx} due to invalid data returned by dataset.\")\n",
        "             continue\n",
        "\n",
        "        # Get original paths using the correct pairs list\n",
        "        try:\n",
        "             raw_image_path, raw_mask_path = inference_pairs[sample_idx]\n",
        "             print(f\"Image Path: {raw_image_path}\")\n",
        "        except IndexError:\n",
        "             print(\"Error: Could not retrieve original path for sample index.\")\n",
        "             raw_image_path = \"Unknown\"\n",
        "\n",
        "\n",
        "        img_tensor = sample['image'].unsqueeze(0).to(device) # Add batch dim and move to device\n",
        "        true_mask = sample['mask'] # Ground truth mask (tensor, H, W)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                output = best_model(img_tensor)['out'] # Get model prediction (1, C, H, W)\n",
        "                pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu() # (H, W) on CPU\n",
        "            except Exception as infer_e:\n",
        "                 print(f\"Error during model inference for sample {sample_idx}: {infer_e}\")\n",
        "                 continue # Skip this sample\n",
        "\n",
        "        # Load original image for better visualization context\n",
        "        original_image_pil = load_image_from_gcs(raw_image_path, gcs_bucket) if raw_image_path != \"Unknown\" else None\n",
        "\n",
        "        # Display\n",
        "        plt.figure(figsize=(18, 6))\n",
        "\n",
        "        # Original Image\n",
        "        plt.subplot(1, 3, 1)\n",
        "        if original_image_pil:\n",
        "            plt.imshow(original_image_pil)\n",
        "            plt.title(f\"Original Image\\n{os.path.basename(raw_image_path)}\")\n",
        "        else:\n",
        "             img_display = sample['image'].permute(1, 2, 0).cpu().numpy()\n",
        "             mean = np.array(imagenet_mean); std = np.array(imagenet_std)\n",
        "             img_display = std * img_display + mean; img_display = np.clip(img_display, 0, 1)\n",
        "             plt.imshow(img_display)\n",
        "             plt.title(\"Input Image (Original Load Failed or Path Unknown)\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Ground Truth Mask\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(true_mask.numpy(), cmap='tab10', vmin=0, vmax=NUM_CLASSES-1)\n",
        "        plt.title(\"Ground Truth Mask\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Predicted Mask\n",
        "        plt.subplot(1, 3, 3)\n",
        "        im = plt.imshow(pred_mask.numpy(), cmap='tab10', vmin=0, vmax=NUM_CLASSES-1)\n",
        "        plt.title(\"Predicted Mask\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Add a colorbar\n",
        "        fig = plt.gcf()\n",
        "        try:\n",
        "            class_names = [\"0:bg\", \"1:bolt\", \"2:busbar\", \"3:cable\", \"4:connector\", \"5:nut\", \"6:film\", \"7:cover\"]\n",
        "            cbar = fig.colorbar(im, ax=plt.gca(), ticks=np.arange(NUM_CLASSES), fraction=0.046, pad=0.04)\n",
        "            cbar.ax.set_yticklabels(class_names[:NUM_CLASSES])\n",
        "        except Exception as cbar_e:\n",
        "             print(f\"Warning: Could not create colorbar: {cbar_e}\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "elif not best_model:\n",
        "    print(\"\\nSkipping inference example because the best model could not be loaded.\")\n",
        "else:\n",
        "     print(f\"\\nSkipping inference example because the {inference_set_name} dataset is empty or unavailable.\")\n",
        "\n",
        "print(\"\\n--- End of Script ---\")"
      ],
      "metadata": {
        "id": "UtFcyHoZaZ25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}